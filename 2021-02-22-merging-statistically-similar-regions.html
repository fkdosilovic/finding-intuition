<!DOCTYPE html><html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type" /><meta content="width=device-width, initial-scale=1" name="viewport" /><!--replace-start-0--><!--replace-start-5--><!--replace-start-8--><title>Merging statistically similar regions - Finding Intuition</title><!--replace-end-8--><!--replace-end-5--><!--replace-end-0--><link href="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.7/dist/semantic.min.css" rel="stylesheet" /><link href="https://fonts.googleapis.com/css?family=Merriweather|Libre+Franklin|Roboto+Mono&amp;display=swap" rel="stylesheet" /><!--replace-start-1--><!--replace-start-4--><!--replace-start-7--><link href="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" rel="icon" /><meta content="Filip Karlo Dosilovic" name="author" /><meta content="Last semester (Fall 2020.) I took a Computer Vision course. The primary reason I decided to attend the class was the course content. Even though many of the (online) computer vision courses today, some made available by popular and high ranking universities, focus on applying deep learning to variou" name="description" /><link href="https://filipk.dosilovic.com/finding-intuition/2021-02-22-merging-statistically-similar-regions" rel="canonical" /><meta content="Merging statistically similar regions" property="og:title" /><meta content="Finding Intuition" property="og:site_name" /><meta content="article" property="og:type" /><meta content="2021-02-22-merging-statistically-similar-regions" property="neuron:zettel-id" /><meta content="2021-02-22-merging-statistically-similar-regions" property="neuron:zettel-slug" /><meta content="computer-vision" property="neuron:zettel-tag" /><meta content="segmentation" property="neuron:zettel-tag" /><script type="application/ld+json">[]</script><style type="text/css">body{background-color:#eeeeee !important;font-family:"Libre Franklin", serif !important}body .ui.container{font-family:"Libre Franklin", serif !important}body h1, h2, h3, h4, h5, h6, .ui.header, .headerFont{font-family:"Merriweather", sans-serif !important}body code, pre, tt, .monoFont{font-family:"Roboto Mono","SFMono-Regular","Menlo","Monaco","Consolas","Liberation Mono","Courier New", monospace !important}body div.z-index p.info{color:#808080}body div.z-index ul{list-style-type:square;padding-left:1.5em}body div.z-index .uplinks{margin-left:0.29999em}body .zettel-content h1#title-h1{background-color:rgba(33,133,208,0.1)}body nav.bottomPane{background-color:rgba(33,133,208,2.0e-2)}body div#footnotes{border-top-color:#2185d0}body p{line-height:150%}body img{max-width:100%}body .deemphasized{font-size:0.94999em}body .deemphasized:hover{opacity:1}body .deemphasized:not(:hover){opacity:0.69999}body .deemphasized:not(:hover) a{color:#808080 !important}body div.container.universe{padding-top:1em}body div.zettel-view ul{padding-left:1.5em;list-style-type:square}body div.zettel-view .pandoc .highlight{background-color:#ffff00}body div.zettel-view .pandoc .ui.disabled.fitted.checkbox{margin-right:0.29999em;vertical-align:middle}body div.zettel-view .zettel-content .metadata{margin-top:1em}body div.zettel-view .zettel-content .metadata div.date{text-align:center;color:#808080}body div.zettel-view .zettel-content h1{padding-top:0.2em;padding-bottom:0.2em;text-align:center}body div.zettel-view .zettel-content h2{border-bottom:solid 1px #4682b4;margin-bottom:0.5em}body div.zettel-view .zettel-content h3{margin:0px 0px 0.4em 0px}body div.zettel-view .zettel-content h4{opacity:0.8}body div.zettel-view .zettel-content div#footnotes{margin-top:4em;border-top-style:groove;border-top-width:2px;font-size:0.9em}body div.zettel-view .zettel-content div#footnotes ol > li > p:only-of-type{display:inline;margin-right:0.5em}body div.zettel-view .zettel-content aside.footnote-inline{width:30%;padding-left:15px;margin-left:15px;float:right;background-color:#d3d3d3}body div.zettel-view .zettel-content .overflows{overflow:auto}body div.zettel-view .zettel-content code{margin:auto auto auto auto;font-size:100%}body div.zettel-view .zettel-content p code, li code, ol code{padding:0.2em 0.2em 0.2em 0.2em;background-color:#f5f2f0}body div.zettel-view .zettel-content pre{overflow:auto}body div.zettel-view .zettel-content dl dt{font-weight:bold}body div.zettel-view .zettel-content blockquote{background-color:#f9f9f9;border-left:solid 10px #cccccc;margin:1.5em 0px 1.5em 0px;padding:0.5em 10px 0.5em 10px}body div.zettel-view .zettel-content.raw{background-color:#dddddd}body .ui.label.zettel-tag{color:#000000}body .ui.label.zettel-tag a{color:#000000}body nav.bottomPane ul.backlinks > li{padding-bottom:0.4em;list-style-type:disc}body nav.bottomPane ul.context-list > li{list-style-type:lower-roman}body .footer-version img{-webkit-filter:grayscale(100%);-moz-filter:grayscale(100%);-ms-filter:grayscale(100%);-o-filter:grayscale(100%);filter:grayscale(100%)}body .footer-version img:hover{-webkit-filter:grayscale(0%);-moz-filter:grayscale(0%);-ms-filter:grayscale(0%);-o-filter:grayscale(0%);filter:grayscale(0%)}body .footer-version, .footer-version a, .footer-version a:visited{color:#808080}body .footer-version a{font-weight:bold}body .footer-version{margin-top:1em !important;font-size:0.69999em}@media only screen and (max-width: 768px){body div#zettel-container{margin-left:0.4em !important;margin-right:0.4em !important}}body span.zettel-link-container span.zettel-link a{color:#2185d0;font-weight:bold;text-decoration:none}body span.zettel-link-container span.zettel-link a:hover{background-color:rgba(33,133,208,0.1)}body span.zettel-link-container span.extra{color:auto}body span.zettel-link-container.errors{border:solid 1px #ff0000}body span.zettel-link-container.errors span.zettel-link a:hover{text-decoration:none !important;cursor:not-allowed}body [data-tooltip]:after{font-size:0.69999em}body div.tag-tree div.node{font-weight:bold}body div.tag-tree div.node a.inactive{color:#555555}body .tree.flipped{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}body .tree{overflow:auto}body .tree ul.root{padding-top:0px;margin-top:0px}body .tree ul{position:relative;padding:1em 0px 0px 0px;white-space:nowrap;margin:0px auto 0px auto;text-align:center}body .tree ul::after{content:"";display:table;clear:both}body .tree ul:last-child{padding-bottom:0.1em}body .tree li{display:inline-block;vertical-align:top;text-align:center;list-style-type:none;position:relative;padding:1em 0.5em 0em 0.5em}body .tree li::before{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{right:auto;left:50%;border-left:solid 2px #cccccc}body .tree li:only-child{padding-top:0em}body .tree li:only-child::after{display:none}body .tree li:only-child::before{display:none}body .tree li:first-child::before{border-style:none;border-width:0px}body .tree li:first-child::after{border-radius:5px 0px 0px 0px}body .tree li:last-child::after{border-style:none;border-width:0px}body .tree li:last-child::before{border-right:solid 2px #cccccc;border-radius:0px 5px 0px 0px}body .tree ul ul::before{content:"";position:absolute;top:0px;left:50%;border-left:solid 2px #cccccc;width:0px;height:1.19999em}body .tree li div.forest-link{border:solid 2px #cccccc;padding:0.2em 0.29999em 0.2em 0.29999em;text-decoration:none;display:inline-block;border-radius:5px 5px 5px 5px;color:#333333;position:relative;top:2px}body .tree.flipped li div.forest-link{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}</style><link rel="preconnect" href="https://fonts.gstatic.com" />
<link
  href="https://fonts.googleapis.com/css2?family=Cormorant+Unicase:wght@700&family=Cormorant:ital,wght@0,600;1,600&family=Montserrat:ital,wght@0,400;0,700;1,400&display=swap"
  rel="stylesheet"
/>

<!-- Based on head.html from: https://github.com/EyebrowHairs/garden -->

<style>
  body {
    background-color: #fdfdfd !important;
  }

  .ui.fluid.container {
    background-color: #fdfdfd !important;
  }

  #zettel-container {
    max-width: 600px !important;
  }

  /* Hide uptree; remove this if you want the uptree to show  */
  /* #zettel-uptree {
        display: none;
         !important;
    } */

  .ui.raised.raised.segment,
  .ui.raised.raised.segments,
  .ui.piled.segment,
  .ui.blue.segment {
    box-shadow: none;
    background-color: #fdfdfd;
    border-radius: 5px;
  }

  .ui.attached.segment,
  .ui.piled.segment,
  .ui.blue.segment {
    border: none;
    background-color: #fdfdfd;
    border-radius: 5px;
  }

  .ui.blue.segment.segment.segment.segment.segment:not(.inverted) {
    border-top: none;
  }

  /* .zettel.view {
      border: none;
    } */

  /* .ui.header+p {
        margin-top: 1.5em;
    } */

  h1 {
    background-color: #fdfdfd !important;
  }

  body p,
  li,
  a {
    font-family: 'Montserrat' !important;
    font-weight: normal;
    font-size: medium;
  }

  /* Hide the query divider */
  /* div.pandoc section.tag-query-results .divider {
        display: none !important;
    } */

  /* div.pandoc section.tag-query-results .ui.list {
        padding-left: 1em;
    } */
</style>
<!--replace-end-7--><!--replace-end-4--><!--replace-end-1--></head><body><div class="ui fluid container universe"><!--replace-start-2--><!--replace-start-3--><!--replace-start-6--><div class="ui text container" id="zettel-container" style="position: relative"><div class="zettel-view"><article class="ui raised attached segment zettel-content"><div class="pandoc"><h1 id="title-h1">Merging statistically similar regions</h1><p>Last semester (Fall 2020.) I took a <em>Computer Vision</em> course. The primary reason I decided to attend the class was the course content. Even though many of the (online) computer vision courses today, some made available by popular and high ranking universities, focus on applying deep learning to various computer vision problems, this class emphasized the traditional approaches to computer vision. Gaussian and Sobel filter, Hough transform, and various other traditional approaches, probably forgotten by computer science students around the world, were the main content of the course. And, I have to say, I enjoyed the class.</p><p>Part of the course’s grade is obtained through the seminar. The theme of my seminar was <em>Merging statistically similar regions</em>. And, in this blog post, I’ll describe the theory behind the approach.</p><p>The rest of this blog post is organized as follows: In the next section, <em>Problem formulation</em>, we formally define the (image) segmentation problem. Afterwards, we will look at how to merge similar regions, where the notion of similarity is defined by comparing statistical properties of regions. The last section is reserved for full derivation of the likelihood ratio.</p><h2 id="problem-formulation">Problem formulation</h2><p>The goal of segmentation is to divide an image into regions, where each region bears unique semantics - such as car, house, etc. . More formally, we assume that the image <span class="math inline">\(I\)</span> is <strong>partitioned</strong> into regions</p><p><span class="math display">$$
I = \bigcup\limits_{i=1}^{N} R_{i} ,
$$</span></p><p>where each <span class="math inline">\(R_i\)</span>, for <span class="math inline">\(i \in \{1, \ldots, N \}\)</span>, represents a region.</p><p>The goal of segmentation is to find those regions, as they might correspond to various objects of interest.</p><p>Today, state-of-the-art results are achieved by deep neural networks, but, as it was mentioned in the introduction, we’ll look at a more traditional approach.</p><h2 id="merging-statistically-similar-regions">Merging statistically similar regions</h2><p>Segmentation based on merging (and splitting) [1] follows the approach presented in the following pseudocode:</p><pre><code class="python language-python"># Create initial segmentation.
regions = create_initial_regions(image)

# Prepare RAG.
rag = create_rag(image, regions)

# Merge similar regions.
while at least one pair is merged:
  for region in rag.regions:
    for neighbour_region in rag.neighborhood(region):
      if similar(region, neighbour_region): 
        rag.merge(region, neighbour_region)</code></pre><p>Region Adjacency Graph (RAG) is a data structure used for manipulating regions in an image and describe a relationship among them. Since we are not concerned with details of inner workings of RAGs in this blog post, for more in depth discussion interested reader is referred to [1].</p><p>We are primarily interested in how to define similarity between two regions.</p><p>We’ll adopt a “probabilitic” approach to image segmentation. We assume that the pixel values in a region have the same intensity and are corrupted by zero mean Gaussian noise.</p><p>Let <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> be regions made of <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span> pixels, respectively. We’ll model our image segmentation problem in the framework of hypothesis testing:</p><ul><li><span class="math inline">\(H_0\)</span>: pixel values from regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> come from the same distribution</li><li><span class="math inline">\(H_1\)</span>: pixel values from regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> come from different distributions</li></ul><p>Our assumption for the null hypothesis is that regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> belong to the same object, while alternative hypothesis suggests that regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> belong to different objects.</p><p>We can test the hypotheses by comparing ratio of their likelihoods</p><p><span class="math display">$$
L = \frac{p(x^{(1)}, x^{(2)}, \ldots, x^{(n + m)} | H_1)}
{p(x^{(1)}, x^{(2)}, \ldots, x^{(n + m)} | H_0)} ,
$$</span></p><p>with some predefined threshold. This test is known as the <strong>likelihood ratio test</strong> [2]. If <span class="math inline">\(L \lt T\)</span>, where <span class="math inline">\(T\)</span> is a predefined threshold, then there is enough evidence supporting the null hypothesis, i.e. regions belong to the same object, otherwise, regions belong to different objects.</p><p>Under the null hypothesis, we assume that the regions belong to the same object, i.e. that the pixels from both regions are sampled from the same distribution</p><p><span class="math display">$$
p(x; \mu_0, \Sigma_0) =
\frac{1}{\sqrt{(2\pi)^{\text{D}}|\Sigma_0|}}
\exp{\left\{ -\frac{1}{2}(x - \mu_0)^{\text{T}}\Sigma_0^{-1}
(x - \mu_0) \right\} } ,
$$</span></p><p>where <span class="math inline">\(D = 3\)</span>, <span class="math inline">\(x, \mu_0 \in \mathbb{R}^3\)</span> and <span class="math inline">\(\Sigma_0 \in \mathbb{R}^{3 \times 3}\)</span>.</p><p>The likelihood of obtaining such a sample equals</p><p><span class="math display">$$
\begin{align}
p(x^{(1)}, \ldots, x^{(n + m)} | H_0)
\stackrel{\text{i.i.d.}}{=}&amp; \prod_{i=1}^{n + m} p(x^{(i)}|H_0) \\
=&amp; \prod_{i=1}^{n + m} \frac{1}{\sqrt{(2\pi)^3|\Sigma_0|}}
\exp{\left\{-D_M\left(x^{(i)}, \mu_0, \Sigma_0\right)\right\}} \tag{1}\ \\
=&amp; \left[(2\pi)^3|\Sigma_0|\right]^{-\frac{n + m}{2}}
\exp{\left\{-\sum_{i=1}^{n + m}D_M\left(x^{(i)}, \mu_0, \Sigma_0\right)\right\}} ,
\end{align}
$$</span></p><p>where <span class="math inline">\(D_M\left( x, \mu, \Sigma\right) =\frac{1}{2} \left(x - \mu\right)^\text{T}\Sigma^{-1}\left(x - \mu\right)\)</span></p><p>After taking care of term inside <span class="math inline">\(\exp\)</span>, i.e. the sum of squared Mahalanobis distances, the likelihood (1) simplifies to</p><p><span class="math display">$$
\tag{1a}
p(x^{(1)}, \ldots, x^{(n + m)} | H_0) = \left[(2\pi)^3|\hat{\Sigma}_0|\right]^{-\frac{n + m}{2}}e^{-\frac{3}{2}(n + m)},
$$</span></p><p>where instead of the determinant of the covariance matrix we used the determinant of the maximum likelihood estimate of <span class="math inline">\(\Sigma_0\)</span>.</p><p>Similar remarks can be made for the likelihood under alternative hypothesis:</p><p><span class="math display">$$
\begin{align}
p(x^{(1)}, \ldots, x^{(n + m)} | H_1)
=&amp; \prod_{i=1}^{n} p(x^{(i)}|H_1) \prod_{j=1}^{m} p(x^{(j)}|H_1) \tag{2} \\
=&amp; \prod_{i=1}^{n} \frac{1}{\sqrt{(2\pi)^3|\Sigma_1|}}
e^{-D_M\left(x^{(i)}, \mu_1, \Sigma_1\right)}
\prod_{j=1}^{m} \frac{1}{\sqrt{(2\pi)^3|\Sigma_2|}}
e^{-D_M\left(x^{(j)}, \mu_2, \Sigma_2\right)}.
\end{align}
$$</span></p><p>Simplifying the likelihood (2) yields</p><p><span class="math display">$$
p(x^{(1)}, \ldots, x^{(n + m)} | H_1) = \left[(2\pi)^3|\hat{\Sigma}_1|\right]^{-\frac{n}{2}} e^{-\frac{3}{2}n}
\left[(2\pi)^3|\hat{\Sigma}_2|\right]^{-\frac{m}{2}} e^{-\frac{3}{2}m} \tag{2a}
$$</span></p><p>where <span class="math inline">\(\hat{\Sigma}_1\)</span> and <span class="math inline">\(\hat{\Sigma}_2\)</span> are maximum likelihood estimates of <span class="math inline">\(\Sigma_1\)</span> and <span class="math inline">\(\Sigma_2\)</span>, respectively, for regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>, under <span class="math inline">\(H_1\)</span>.</p><p>Finally, combining (1a) and (2a) into the likelihood ratio yields</p><p><span class="math display">$$
\tag{3}
L = \frac{|\hat{\Sigma}_0|^{\frac{n + m}{2}}}{|\hat{\Sigma}_1|^{\frac{n}{2}}|\hat{\Sigma}_2|^{\frac{m}{2}}}
$$</span></p><h2 id="deriving-the-likelihood-ratio-for-rgb-images">Deriving the likelihood ratio for RGB images</h2><p>This section provides full derivation of likelihood ratio (3) for RGB images. Deriving the likelihood ratio for grayscale images is similar but won’t be covered here. Instead, intersted reader is referred to [1].</p><p>As it was mentioned in the introduction, we assume that the image <span class="math inline">\(I\)</span> is <strong>partitioned</strong> into regions</p><p><span class="math display">$$
I = \bigcup\limits_{i=1}^{N} R_{i} ,
$$</span></p><p>where each <span class="math inline">\(R_i\)</span>, for <span class="math inline">\(i \in \{1, \ldots, N \}\)</span>, represents a region.</p><p>For each region, we assume that pixel values (intensity) are constant, but corrupted by zero mean Gaussian noise.</p><p>Let <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> be regions made of <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span> pixel, respectively. We’ll test the following hypothesis:</p><ul><li><span class="math inline">\(H_0\)</span>: pixel values from regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> come from the same distribution</li><li><span class="math inline">\(H_1\)</span>: pixel values from regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> come from different distributions</li></ul><!-- Our assumption for the null hypothesis is that regions $R_1$ and $R_2$ belong to
the the same object, while alternative hypothesis says that regions $R_1$ and
$R_2$ belong to different objects. -->
<p>We can test the two hypothesis by comparing the ratio of their likelihoods</p><p><span class="math display">$$
L = \frac{p(x^{(1)}, x^{(2)}, \ldots, x^{(n + m)} | H_1)}
{p(x^{(1)}, x^{(2)}, \ldots, x^{(n + m)} | H_0)} ,
$$</span></p><p>with some predefined threshold.</p><p>The null hypothesis assumes that regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> belong to the same object, i.e. that the pixels are sampled from the same distribution</p><p><span class="math display">$$
p(x; \mu_0, \Sigma_0) =
\frac{1}{\sqrt{(2\pi)^{\text{D}}|\Sigma_0|}}
e^{-D(x, \mu_0, \Sigma_0)} ,
$$</span></p><p>where <span class="math inline">\(x, \mu_0 \in \mathbb{R}^3\)</span> and <span class="math inline">\(\Sigma_0 \in \mathbb{R}^{3 \times 3}\)</span>.</p><p>The likelihood under <span class="math inline">\(H_0\)</span> can be written as</p><p><span class="math display">$$
\begin{align}
p(x^{(1)}, \ldots, x^{(n + m)} | H_0)
\stackrel{\text{i.i.d.}}{=}&amp; \prod_{i=1}^{n + m} p(x^{(i)}|H_0) \\
=&amp; \prod_{i=1}^{n + m} \frac{1}{\sqrt{(2\pi)^3|\Sigma_0|}}
e^{-\frac{1}{2}(x^{(i)} - \mu_0)^\text{T}\Sigma_0^{-1}(x^{(i)} - \mu_0)} \\
=&amp; \left[(2\pi)^3|\Sigma_0|\right]^{-\frac{n + m}{2}}e^{-\frac{1}{2}\sum_{i=1}^{n + m}(x^{(i)} - \mu_0)^\text{T}\Sigma_0^{-1}(x^{(i)} - \mu_0)}
\end{align}
$$</span></p><p>We have to simplify the exponent:</p><p><span class="math display">$$
\begin{align}
\sum_{i=1}^{n + m}(x^{(i)} - \mu_0)^\text{T}\Sigma_0^{-1}(x^{(i)} - \mu_0)
&amp;= \sum_{i=1}^{n + m}\text{Tr}\left[(x^{(i)} - \mu_0)^\text{T}\Sigma_0^{-1}(x^{(i)} - \mu_0)\right] \\
&amp;= \sum_{i=1}^{n + m}\text{Tr}\left[(x^{(i)} - \mu_0)(x^{(i)} - \mu_0)^\text{T}\Sigma_0^{-1}\right] \\
&amp;= \text{Tr}\left[\sum_{i=1}^{n + m}(x^{(i)} - \mu_0)(x^{(i)} - \mu_0)^\text{T}\Sigma_0^{-1}\right] \\
&amp;= \text{Tr}\left\{\left[\sum_{i=1}^{n + m}(x^{(i)} - \mu_0)(x^{(i)} - \mu_0)^\text{T}\right]\Sigma_0^{-1}\right\} \\
\end{align}
$$</span></p><p>Expression in the brackets is un-normalized empirical estimate of the covariance matrix. We can write</p><p><span class="math display">$$
(n + m)\text{Tr}\left[\left(\smash{\underbrace{\frac{1}{n + m}\sum_{i=1}^{n + m}(x^{(i)} - \mu_0)(x^{(i)} - \mu_0)^\text{T}}_{\hat{\Sigma}_0}}\right)\Sigma_0^{-1}\right]
$$</span></p><br />
<p>which yields</p><p><span class="math display">$$
\sum_{i=1}^{n + m}(x^{(i)} - \mu_0)^\text{T}\Sigma_0^{-1}(x^{(i)} - \mu_0) = (n + m)\text{Tr}\left[\hat{\Sigma}_0\Sigma_0^{-1}\right].
$$</span></p><p>Since <span class="math inline">\(\hat{\Sigma}_0\)</span> is an empirical estimate of <span class="math inline">\(\Sigma_0\)</span> and under the assumption that <span class="math inline">\(H_0\)</span> is true, we can write <span class="math inline">\(\hat{\Sigma}_0\Sigma^{-1}_0 = \text{I}_3\)</span>, which yields</p><p><span class="math display">$$
\tag{4}
\sum_{i=1}^{n + m}(x^{(i)} - \mu_0)^\text{T}\Sigma_0^{-1}(x^{(i)} - \mu_0) = 3(n + m).
$$</span></p><p>Taking into account (4), we can write the final form of the likelihood <span class="math inline">\(p(x^{(1)}, \ldots, x^{(n + m)} | H_0)\)</span> as</p><p><span class="math display">$$
\tag{5}
p(x^{(1)}, \ldots, x^{(n + m)} | H_0) = \left[(2\pi)^3|\hat{\Sigma}_0|\right]^{-\frac{n + m}{2}}e^{-\frac{3}{2}(n + m)},
$$</span></p><p>where we replaced the unknown covariance matrix <span class="math inline">\(\Sigma_0\)</span> with its empirical estimate <span class="math inline">\(\hat{\Sigma}_0\)</span>, and <span class="math inline">\(|\hat{\Sigma}_0\)</span>| denotes the determinant of <span class="math inline">\(\hat{\Sigma}_0\)</span>.</p><p>Similar remarks can be made for likelihood under the alternative hypothesis:</p><p><span class="math display">$$
\begin{align}
p(x^{(1)}, \ldots, x^{(n + m)} | H_1)
=&amp; \prod_{i=1}^{n} p(x^{(i)}|H_1) \prod_{j=1}^{m} p(x^{(j)}|H_1) \\
\end{align}.
$$</span></p><p>The final expression for the likelihood under alternative hypothesis is</p><p><span class="math display">$$
\tag{6}
p(x^{(1)}, \ldots, x^{(n + m)} | H_1) = \left[(2\pi)^3|\hat{\Sigma}_1|\right]^{-\frac{n}{2}} e^{-\frac{3}{2}n}
\left[(2\pi)^3|\hat{\Sigma}_2|\right]^{-\frac{m}{2}} e^{-\frac{3}{2}m},
$$</span></p><p>where <span class="math inline">\(\hat{\Sigma}_1\)</span> and <span class="math inline">\(\hat{\Sigma}_2\)</span> are empirical estimates of <span class="math inline">\(\Sigma_1\)</span> and <span class="math inline">\(\Sigma_2\)</span>, respectively, for regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>, under <span class="math inline">\(H_1\)</span>.</p><p>Finally, we obtain the likelihood ratio from (5) and (6)</p><p><span class="math display">$$
L = \frac{|\hat{\Sigma}_0|^{\frac{n + m}{2}}}{|\hat{\Sigma}_1|^{\frac{n}{2}}|\hat{\Sigma}_2|^{\frac{m}{2}}}.
$$</span></p><h2 id="references">References</h2><p>[1] Jain, Ramesh C., Kasturi, Rangachar and Schunck, Brian G.. Machine vision.. : McGraw-Hill, 1995.<br />[2] Wasserman, Larry. <em>All of statistics : a concise course in statistical inference</em>. New York: Springer, 2010.</p></div><div class="metadata"><div class="date" title="Zettel date"><time datetime="2021-02-22">2021-02-22</time></div></div></article><nav class="ui attached segment deemphasized bottomPane" id="neuron-tags-pane"><div><span class="ui basic label zettel-tag" title="Tag">computer-vision</span><span class="ui basic label zettel-tag" title="Tag">segmentation</span></div></nav><nav class="ui bottom attached icon compact inverted menu blue" id="neuron-nav-bar"><!--replace-start-9--><a class="item" href="." title="Home"><i class="home icon"></i></a><!--replace-end-9--><a class="right item" href="impulse" title="Open Impulse"><i class="wave square icon"></i></a></nav></div></div><!--replace-end-6--><!--replace-end-3--><!--replace-end-2--><div class="ui center aligned container footer-version"><div class="ui tiny image"><a href="https://neuron.zettel.page"><img alt="logo" src="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" title="Generated by Neuron 1.9.31.0" /></a></div></div></div></body></html>